const post = {};
let title = 'Title';
let content = 'Contet'
post[1]={title,content};

//Result:
// {title: 'Title', content: 'Contet'}
const obj = {
    'prop1': '123',
        'prop2': '123'
    };
    

1. docker run hello-world

Client is contacting Docker Deamon, and docker Deamon is reaching the docker server.
~~Question - what is the Docker Deamon?

Docker server is checking if image is actually in a 'Image cache' on local machine
If empty - reach Docker Hub (free repo)

Container is an instance of Image

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

2. What the container is?
Brief explanation of OS - Kernel, CPU, Memory, Hard Disk
Kernel - running software process

If we'd like to save a file from Node.js to the hardrive, this operation happens through the Kernel
Processess running on our computers (e.g Chrome Spoti Node) interact with Kernel through System Calls


Namespacing - this is the feature served by OS, which allows you to segment portions of those resources
If we using namespacing, then Kernel needs to know which process is making the system call - based on that information it can select accurate segment of HD

Control Groups - Limit amount of resources used per process
We can control for example Memory, CPU Usage, HD I/O, Network Bandwith

CONTAINER - process or set of processes that have a grouping of resources specifically assigned to it

Image - Snapshot of files and Startup Command
What happens when image is turned into container?
First off, the kernel is going to isolate a little section of the harddrive and make it available to just this container.
image is taken and placed into that little segment of the hard drive.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

3. When we are running Docker we are just running the Linux VMs on our computers, as Namespacing and Control Groups are used by Linux, not VM or MacOS either
 

 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

 4. Docker CLI:
#CREATING AND RUNNING A CONTAINER FROM AN IMAGE + overwriting default docker command
$docker run {imagename} {command}

#We may run these two commands as these commands exist inside busybox fs snapshot. We can't execute them from hello-world container
$docker run busybox ls
$docker run busybox echo hi there


#ALL RUNNING CONTAINERS
$docker ps 

#HISTORY OF RUNS
$docker ps --all

LIFECYCLE OF THE CONTAINER
docker run = docker create + docker start

#Create Container
$docker creat hello-world {return guid}

#Start Container
$docker start -a <paste guid>

$-a  === watch docker output from the container and print output to the console

#We can run again container with status 'exited'
#Let's say we've run our docker container like the following:
$docker run busybox echo hello there
#also $docker run -it busybox sh
#This container has status 'exited'.
#However, we can run it again, by using
$docker start -a <guid>
#As an output, we will receieve 'hello there' again, as this is the process which has been invoked with a container

#This delete all docker history - exited, and also docker catche
$docker system prune 

#get information from the container, get all logs emmited by this container
$docker logs

#hardware signal is going to the process SIGTERM. Stop gives you a bit of time, cleanup etc. Kill shut it down immidiately
$docker stop <container id> ==> SIGTERM
$docker kill <container id> ==> SIGKILL

#Run command inside the container 
# -it flag allow us to put input to the container
$docker exec -- execute an additional command in a container
$docker exec -it -- allows us to provide input to the container
$docker exec -it <container id> <command i.e. redis-cli>

#Every container(every linux process) has 3 communication channels attached to it - STDIN STDOUT STDERR
#-i flag allow us to attach our terminal to STDIN proces inside our container
#-t formatted manner

#Open shell in context of your running container
$docker exec -it <guid> sh    <--- run container
$sh - command processor, shell
$ctrl+d || exit = exit


#Container isolation

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
***CREATE IMAGE***
//1. Dockerfile
//2. Docker client(docker cli)
//3. Docker server creates an image

Creating a Dockerfile:
1. Base image
2. Run some commands to install additional programs
3. Specify a command to run on container startup

#After image created
#Take docker file and create img out of cli
$docker build .

#Next
$docker run <id_from_docker_build>


#Analogy for creating Dockerfile
#We are using Alpine, which is linux distribution
#Alpine img as any other img has snapshot of file structure and startup command

#This has nothing to do with docker - it is apache package something, which is apache package manager which allow us to upload redis from 
$FROM alpine


#Look back at the previous step (base img). Take this img and create container based on it. Next, run RUN apk~~ inside container (primary running process)
$RUN apk add --update redis 
$CMD ["redis-server"]


#RECAP
#1. Install alpine as a base image
#2. Run command in temporarly created container based on alpine img
#3. Container with new FS (with redis) there - take snapshot of it, and shut down container
#4. Get copied snapshot
#5. CMD says - well, if you'll ever need to run this container I'd like you to run this first CMD first
#6. In result we've got container with modified primary command
#7. Shut it down, take a pic
#8. Output of DOCKERFILE is the last img generated in file




//TO ADD TAG:
//1. dockerid/reponame:version
$docker build -t stephengrinder/redis:latest .

//2. After this all you need to do is run
$docker run stephengrinder/redis
 
#########582 lesson

//CREATE AN IMAGE BASED ON CONTAINER
//1. Run container: $docker run -it alpine sh
//2. Manually install redis: $apk --update redis
//3. We've changed file system. Assign default command to it through docker cli
//4. $docker commit -c "CMD 'redis-server'"  CONTAINERID
//5. -c => default command


//PLANNED ERRORS
//PORT FORWARDING - CONTAINER PORT CANNOT BE ACCESSED DIRECTLY, IT HAS TO BE FORWARDER THROUGH THE CONTAINER
//It is a runtime constains, so port configuration is done along with 'RUN' command

//docker run -p 8080[INCOMING REQUEST TO THE LOCALHOST] : 8080 [PORT SPECIFIED INSIDE THE CONTAINER] <IMG ID>

#Kubernetess Cluster - set of VMs - VM is Node
#All managed by MASTER

#REMINDER FOR DOCKER commands
#docker build -t 10eggs/posts

~~~~~~~~~~~~~~~~~~~~~~~~K8S~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Node - VM
#Pod - container
#Service - Run container in cluster - handling networking.
#It abstracts away all the difficulties of trying to figure out what ip or what port some given program is running on

$ctrl+` - focus on terminal
$ctrl+1 - focus window


#####LIST OF K8s world
#Run first pod:
$kubectl apply -f posts.yaml
$kubectl get pods
$kubectl exec -it [pod_name][cmd]
$kubectl logs [pod_name]
$kubectl delete pod [pod_name]
$kubectl describe pod [pod_name]

#Deployment - if pod dissapear - recreate, it manages set of pods
#If updated version of pod is ready, then deployment object carry on all replacement tasks

#To Create Deployment
$kubectl apply -f posts-depl.yaml

#To Check Deployments
$kubectl get deployments

#Updating the Image Used by a Deployment
#Method 1
1. Make change to project code,
2. Rebuid the image, specifying a new image version,
3. In the deployment config file, update the version of the image
4. Run the command kubectl apply -f [depl file name]
#Kube knows what was changed - if deployment already exist, you'll see 'configured' inormation rather than 'created'


#Method 2
1. The deployment must be using the 'latest' tag in the pon spec section
2. Make an update to your code
3. Build the image
4. Push the image to docker Hub
5. Run the command kubectl rollout restart deployment [depl_name]
#Kube says restarted


#NETWORKING WITH SERVICES(services as an objects in kubectl)
#4 different types
#Cluster IP - in the cluster
#Node Port - outside cluster, usually for dev purposes
#Load Balancer - outside cluster, prod purpose
#External Name - (out of scope)


#NodePorts - port,targetport(pod port),nodePort (node port)
#When posts-srv is created, we need to apply it to kubectl

#U can create separate yaml for ClusterIP or just colocate with depl yaml

#For services ClusterIP is deafult service name, that's why we can skip it when we are doing our job
#We are separating different objects by putting '---' in yaml


#When we've got services for our pods we'll need to change url from localhost to name of our ClusterIP services, e.g. 'posts-clusterip-srv'


#TO APPLY MULTIPLE CONFIG FILES BY ONCE:
$kubectl apply -f . 


kubectl delete svc comment-srv

#Start with React application


#LET's GO WITH LOAD BALANCER

#LBS - tells Kubernetes to reach out to it's PROVIDER and PROVISION a load balancer. Gets traffic in to a single pod
#LBS has different behavior than pods,services,deployments - it is going to tell our cluster to reach out to its Cloud Provider.
#It is asking Cloud Provider to provide Load Balancer for us

 
#Ingress - a pod with a set of routing rules to distribute traffic to other services. Ingress Controller is feed by config file
#Need to trick our machine that posts.com = localhost. Whenever we want to access posts.com - we'll be always redirected to localhost



#Routing rules for mcsv for cluster
#Ingres engine module cannot do routing based upon the metod of request, so we need to specify when we'd like to use post/get/put/delete
#Nginx doesnt support wildcard - need to use regular expressions
#nginx.ingress.kubernetes.io/use-regex: 'true'

#Path to add 127.0.0.1 posts.com
#C:\Windows\System32\drivers\etc\hosts

02/09/2022 14:00 - lesson 101
#ATM making changes in our code is a real pain
#Introducing Skaffold
#Few examples of what Skaffold does for us:
    - Automates many tasks in dev env,
    - Makes it really easy to update code in a running pod
    - Makes it really easy to create/delete all objects tied to a project at ONCE


#skaffold.dev

#We need to add another config file to tell scaffold how to manage all subprojects in our cluster
#it's not applied to kubernetess - it is running outside of our cluster

#To run skaffold
$skaffold dev

#To stop skaffold - ctrl+C

map object to store all activities

//Routing
Outlet - router component
Router outlet component is swapping outlet with required component
Depends on route our component will be swapped

//Reseting state with the key:
https://beta.reactjs.org/learn/preserving-and-resetting-state#option-2-resetting-state-with-a-key

For some cases we'd like to reset state of the component rather than preserving it.
To do so we can use keys (same property as we are using for react lists)
We need to add key to our router, as we are rendering same component for two different routes, we'd like to distinguish them by adding key property

//Children saved in router are replaced in outlet
//useLocation() - another react hook from react-router

//https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/reduce

//<> - fragment shortcut

//ApiController attribute
//400 if it encounters error

//Exceptions are heavy object - we should try to avoid them where we can do it

//FluentValidator nuget package
//https://docs.fluentvalidation.net/en/latest/
//We have two options - one is to validate every single domain model, second is to add validator to the pipeline (we are opt to second option)
//ApplicationServiceExtensions have to have two extra registered middlewares:

//AddFluentValidationAutoValidation();
//AddValidatorsFromAssemblyContaining<Create>();

//We need to specify class where we are going to perform our validation (our case ActivityValidator)
//Inside Command class we are specifying that validator is against Command instance (not activity), and next we are assigning validator to the 
//x.Activity property

//In controller we are returning IActionResult now (instead of previous ActionResult)
//IActionResult allow us to return http responses instead of type of thing

//To handle exceptions we are introducing Result class in application/core/result
//So our MediatR is now working with Result objects instead of Activities objects.

//We may have validations against success/null inside ActivitiesController or to make it thiner we can extract it to the base controller
//SaveChangesAsync in dbcontext return values. If nothing was written to the database then result will be 0, otherway positive integer.

//For delete handler we are going to do additional check in base controller against null value;
//Note for exceptions - in minimal API we don't need to use UseDeveloperExceptionPage() method as this is set by default.
//That's why for every incorrect request we can see a detailed exception message in our postman.
//In prod environment we'd like to disable this 

//We want to handle exception no matter what environment is in Usage
//To do that we'll introduce our own exception handling middleware

//https://learn.microsoft.com/en-us/aspnet/core/fundamentals/middleware/?view=aspnetcore-7.0
//Middleware layer is looking for InvokeAsync method, nothing else, that's why for custom middlewares we need to have our logic inside this method

//The reason why we are putting contenttype in our middleware is that we are not in the context of controller (it sets content type to app/json automatically)


//In axios everything what is coming from server with other resp status than 200 will be rejected on promise level

// By using <code> tag we are just injecting html

//TO CLEANUP DB:
//1. From solution path:
//-s flag = startup project, -p persistance   
//2. dotnet ef database drop -s API -p Persistance


//Introducing FORMIK
//Formik provides render properties
//Destructuring properties we are intrested to get from Formik by {({})}

//ctrl+shift+l - select all words

//Partial<className> making all properties optional

//$npm ls date-fns - is listing installed packages with selected name
//loadash - simplify javascript

//When we want to do migration  - stop API proj
//.AddEntityFrameworkStores<DataContext>(); <- this allow us to query user by EntityFramework

//JWT - three ingredients
//1. Header - alghoritm which was used to encrypt signature of the Token
//2. Payload - What we are going to send back inside this token. We can add claims about the user
//you may put it in clear text - did not put any confidential information inside the token

// This free properties are incldued in TokenPayload (we are usually adding username as well). These three dates gives us information
// about token
//nbf - no before - when can this token start being use
//exp - latest date this token up until
//iat - issued at - similar to nbf

//3. Signature - our server will sign token before it leaves the server
//Server is going to use token key that stored on the server and never leaves the server
//What we are using in JWT is encrypting signature (symetric security), same key for encrypting/decrypting
// key is a very long string to sign our token with

// Local storage - in client (browser) is the place where we are storying data like JWT token received from auth server
// Local storage is persisten storage which means even when we turn off the browser our token will be there
// Then auth token is sending with every request in a header Authentication. We are passing this token as a Bearer Token

// Service - something that you creating that doesn't involve data access, there is no repository
// we don't want to put this kind of logic directly inside controller - we'd like to keep it separate

// IdentityServer - add SCOPED service - this token service will be scoped just to http request itself
// This is a common scenario - when we are creating service by ourselves we typically use AddScoped - when http request is finished the service is disposed

//Other method is AddTransient - service specifically created for the method
//https://jwt.ms/

//How to store secrets in ASP.NET Core app in dev model
//https://learn.microsoft.com/en-us/aspnet/core/security/app-secrets?view=aspnetcore-7.0&tabs=windows
//Environment variables [x]
//Secret manager
//appsettings.json 
// for 3rd party API keys I would use .net user secrets

//Difference between ExtensionMethod class and other class - for first one we are not initializing our config inside class - we are just
//using method parameters, however when we are going to use config in other class we need to initialize this field first.

//We can use [Authorize] attribute above every controller method signature (every endpoint)
//Or we can use Authorisation in Program class, to set all endpoints default in Services.AddController

//IdentityServiceExtensions - need to apply something to detect email or user
//In controller we are checking if username or email has been already taken. Password validation - we are leaving it for IdentityServer
//For now blank email or username will be accepted by API. We need to make sure that this validate before we allow user to register.

//We are using fluent validation in our project so far, but 
//?=.*\\d any character, one of them needs to be digit


//In get user we need to have an acceess to email token or email claim from our token
//Inside our Controller context we've got an access to User object. 

//User object - claim principal associated with User executing an action
//

//Allow annonymous take precedence before every other attribute ihen it is declared at the controller level
//When we are adding store to our component we should instantly export it as an observer

//We don't need to create loading indicator when we are using isSubmitting because forming is recognizing that we are using onSubmit,
//and it is turning on/off isSubmitting when it receive a response from onSubmit

//In formik, when you are surround <values> which are parameters of onSubmit method with parenthesis, then we can get some methods and properties
//in parameters as well, we just need to use {}, i.e.
// onSubmit ={(values, {methods,andProps, i.e. setErrors}) => userStore.login(values)}

//When we are using setErrors in onSubmission, we also need to add additional property to our initial values form Formik component
//initially errors are set to null

//Mobx
//Reaction -  we can react on observable changes in our store

//1. Reaction - first param is about what are we going to react
//2. Second - how to deal with this

//For errors if we are using same values for key and value we can have a shortcut for it, e.g. {error:error} = {error}


//We have this ability to do N:N table in EF out of the box.
//All we need to do is add two ICollections<JoinedType> in both classes.

//We are going to implement N:N by ourselves, to do that instead of ICollection<JoinedType> we'll use ICollection<JoinTableType> (ActivityAttendee rather than Activity or AppUser)

//To add additional configuration to the entity framework we need to override OnModelCreating method from identitydbcontext

//For ActivityAttendee table we'd like to have primary key which is combination of both - AppUser PK and Activity PK

//dotnet ef migration add ActivityAttendee -p Persistance -s API
//dotnet new classlib -n Infrastructure
//dotnet sln add Infrastructure
//dotnet add Reference Application

//160 - Object reference not set to an instance of the object
//To avoid it (in our case problem with N:N where N is not initialized as an empty array) we can just initialize new array
//Loading related data
//Eager,Lazy,Explicit

//161. Object cycle dependend - infinitive loop of relationship between two entities
//We can disable it or we can reshape our data

//Reshape is done by introducing ActivityDto. In our MediatR handler we'd like to return ActivitiesDto set rather that Activities.
//To be able to do that we need to introduce mapping layer by using AutoMapper

//Automapper is returning empty arrays as it is not smart enough to recognize related properties if their names don't match.
//For Member - destination member

//It's worth to note that when we are selecting from our database we are including many unnecessary properties.
//To avoid it instead of using eager loading we can use PROJECTION
//Projection allow us to skip 'Include' part of the EF Core query

//First or default vs SingleOrDefault
//If one or more matches then SingleOrDefault throw exception, where FirstOrDefault returns null;

//165. Adding a custom auth policy
//Our ActivityAttendee primary key is made of combination of user & activity PK

//We discovered a bug where hostUserName and attendees are cleaned-up after editing.
//Interesting note is that our IsHostRequirementHandler has a transient lifecycle, and even if it is disposed (after http call)
//the entity tracked by this handler (we are calling ef from handler) is not disposed at all (it is staying in memory).

//FindAsync is always tracking an entity

//FindAsync vs FirstOrDefaultAsync()

//To be able to update seed data first we need to use ef tool to drop actual database and perform another migration

//170. Adding attendance component
F2 - VS Code to change many occurences

//some() returns true if the callback function returns true for any element of the array

//PROBLEM - Dto object properties have C# naming convention for Propertines, i.e. DisplayName, UserName.
//DTO returned by controller, and props name was changed to js syntax, e.g. displayName, userName
//Had to introduce another property to user interface in typescript


//Update existing activityh with new values by using spread operator
let updatedActivity = {...this.getActivity(activity.id), ...activity}
...activity override exisitng props

services.Configure - what it does?
IFormFile - file which is transfeered by http request
Crop image?
